icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup to a micro-service architecture, adding concurrency
        control, cancel/resume, better resiliency across restarts, and steadier node-agent
        resource usage.', 'fs-backup now supports Windows workloads, enabling full
        Windows backup/restore scenarios when combined with CSI snapshot data movement
        from 1.16.', 'Added (beta) Kubernetes Volume Group Snapshot support for both
        CSI snapshot backups and CSI snapshot data movement, enabling point-in-time,
        write-order-consistent snapshots across multiple volumes.', 'Added PriorityClass
        support across Velero components (server, node-agent, data mover pods, and
        repo maintenance jobs) so you can control scheduling/eviction priority.',
      'Improved data mover scalability/resiliency with a configurable node-agent PrepareQueueLength
        to prevent creating excessive pending pods, plus better restart/orphan handling
        and enhanced node-selection (including per-storage-class) for CSI snapshot
        data movement restores.', Extended Resource Policies with include/exclude
        policy support so reusable resource filters can be defined in a policy ConfigMap
        and applied across multiple backups.]
    breaking_changes: ['Restic-based fs-backup is removed: --uploader-type=restic
        is no longer a valid install configuration; restores from existing restic
        backups remain supported only until v1.19.', 'Repository maintenance job settings
        were removed from Velero server flags and moved to a ConfigMap; the following
        flags no longer exist: --keep-latest-maintenance-jobs, --maintenance-job-cpu-request/limit,
        --maintenance-job-mem-request/limit.', 'PVC restore behavior change: when
        no node mapping exists, Velero now always removes the PVC selected-node annotation
        (previously it could be preserved if the node existed).']
  chart_version: 11.3.2
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup to a micro-service architecture, enabling better
        concurrency control, cancellation, and resilience to node-agent restarts while
        stabilizing node-agent resource usage.', 'Added fs-backup support for Windows
        workloads by running data mover pods on Windows nodes, completing Windows
        backup/restore coverage alongside CSI snapshot data movement.', Introduced
        beta Kubernetes Volume Group Snapshots support for CSI snapshot backup and
        CSI snapshot data movement to enable point-in-time consistency across multiple
        related volumes., 'Added PriorityClass support across Velero components (server,
        node-agent, data movers, and maintenance jobs) so operators can control scheduling
        importance.', 'Improved data mover scalability and resiliency with a node-agent
        PrepareQueueLength to throttle pod creation, better restart/orphan handling,
        and restore node-selection including per-storage-class rules.', Extended resource
        policy capabilities with reusable include/exclude filters (includeExcludePolicy)
        in addition to volumePolicy.]
    breaking_changes: ['Restic uploader is removed for new fs-backup operations in
        v1.17; --uploader-type=restic is no longer a valid install configuration,
        though restores from existing restic-path backups are supported only until
        v1.19.', Repository maintenance job configuration flags were removed from
        the velero server (--keep-latest-maintenance-jobs and maintenance job resource
        request/limit flags) because these settings moved to a ConfigMap., 'PVC restore
        behavior changed: selected-node annotation is now always removed when no node
        mapping exists; previously it could be preserved if the node existed (noted
        as a breaking change in the release notes).']
  chart_version: 11.3.2
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ['Velero server, node-agent, data mover, and maintenance jobs gain
        Windows-node compatibility (hybrid Linux/Windows clusters).', Backup engine
        now supports parallel ItemBlock processing via a worker pool controlled by
        a new server flag., 'CSI snapshot backup/restore no longer retains/restores
        unnecessary VolumeSnapshotContent objects, reducing cross-cluster noise when
        sharing a BSL.', 'BackupRepository maintenance adds history/status reporting,
        better restart recovery, and behavior changes for readOnly BSLs.', 'Velero
        image packaging changes: velero, velero-helper, and velero-restore-helper
        are merged into a single Velero image.']
    features: ['Windows cluster support: Velero components can schedule onto Windows
        nodes and data mover supports Windows workloads end-to-end, with documented
        limitations (no fs-backup for Windows, no security descriptors/NTFS xattrs).',
      Parallel ItemBlock backups improve backup throughput for large resource counts;
        pre/post hooks run within item blocks and can execute in parallel. Configure
        via --item-block-worker-count (default 1)., 'Data mover restore scalability:
        new node-agent config flag ignoreDelayBinding allows distributing restores
        for WaitForFirstConsumer volumes across nodes instead of being pinned to the
        attached node.', 'Observability improvements: additional logging around intermediate
        objects and cleanup failures for data mover operations, enabled by default
        in node-agent logs.', 'Backup repository maintenance enhancements: RecentMaintenance
        history in BackupRepository CR, recapture running jobs after server restart,
        configurable fullMaintenanceInterval (normalGC/fastGC/eagerGC), and avoidance
        of maintenance/init on readOnly BSLs.', "Usability improvement for CSI snapshots:\
        \ removes retained VolumeSnapshotContent from backups so it won\u2019t be\
        \ synced/restored unnecessarily.", Volume Policy now supports filtering volumes
        by PVC labels., Object-level resource status restore control via per-object
        annotation velero.io/restore-status., 'Restore helper binary consolidated
        into the main Velero image, simplifying image management.']
    breaking_changes: ['Operational change: restore-helper is no longer a separate
        image; update any manifests/automation that referenced a distinct velero-restore-helper
        image to use the unified Velero image.', 'Behavior change: backup repository
        maintenance and repository initialization are skipped for readOnly BackupStorageLocations;
        workflows that relied on those operations against readOnly BSLs must be adjusted.',
      'Backup behavior change: retained VolumeSnapshotContent objects are no longer
        included in backups; environments that depended on restoring those objects
        explicitly may see different restore artifact sets.']
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ["Data mover architecture change: CSI Snapshot Data Movement now runs\
        \ in dedicated per-operation backupPod/restorePod \u201Cdata mover micro service\u201D\
        \ pods instead of node-agent pods, improving security (no hostPath), resource\
        \ isolation, and resilience.", 'New configuration knobs via ConfigMaps: node
        selection for repository maintenance jobs; backup PVC behavior (read-only
        mounts and storageClass selection for data mover pods); and per-repository
        cache size limits to control ephemeral storage usage.', 'Performance and stability
        improvements: fixed a server memory leak after plugin calls; velero client
        QPS/burst settings now propagate to plugins to reduce API throttling; Kopia
        maintenance memory usage improved via upstream fixes.']
    breaking_changes: ["Restic uploader path is now deprecated for filesystem backups;\
        \ backups/restores still succeed in 1.15 but emit warnings when restic is\
        \ selected/used\u2014plan migration away from restic.", node-agent configuration
        ConfigMap name is no longer fixed; if you use node-agent configuration you
        must pass the name via the `node-agent-configmap` server parameter., Repository
        maintenance job settings are moving from server flags to a new repository
        maintenance ConfigMap (ConfigMap values take precedence); the old flags remain
        for backward compatibility but should be migrated., PVC selected-node change
        feature is deprecated and will be removed in a future release; avoid depending
        on it., 'Known issue: read-only backup PVCs may fail on SELinux clusters unless
        you enable a super-privileged mode that disables SELinux controls for the
        backupPod container (may conflict with PSA/security policies).']
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Improved backup/restore volume handling: Volume Policies now support
        explicit actions (fs-backup vs snapshot), and restore operations persist VolumeInfo
        metadata and show more CSI snapshot restore details in `velero restore describe`.',
      'Stability/performance improvements: restore workflow gains a new Finalizing
        phase, and kopia/restic repository maintenance is moved into separate Kubernetes
        Jobs to reduce Velero server pod OOM risk.', 'Operational controls: node selection
        for data-movement (datamover) pods can be configured via ConfigMap; parallel
        restore is now configurable; increased k8s client QPS/burst to reduce throttling.']
    breaking_changes: [CSI plugin is now merged into the Velero repo and installed
        by default as an internal plugin; you must stop installing the CSI plugin
        via `--plugins` during `velero install` to avoid conflicts/duplication., 'Node-agent
        default resource requests/limits were removed, making pods BestEffort by default;
        you may need to explicitly set requests/limits to avoid eviction/throttling
        in busy clusters.', 'Namespace filtering during backup changes when `includedNamespaces`/`excludedNamespaces`
        are unset but label selectors are used: only namespaces containing selected
        resources are backed up (previously all namespaces were included).', 'PV patching
        in the new Restore Finalizing phase can cause restores to end `PartiallyFailed`
        if PVs are stuck Pending, where they might have been marked Complete before.']
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Resource Modifiers now support JSON Merge Patch and Strategic Merge
        Patch in addition to JSON Patch, enabling more flexible restore-time substitutions
        using the same ConfigMap ruleset.', 'Node-agent concurrency controls let you
        cap how many data-movement activities (fs-backups, CSI snapshot data movement)
        run per node globally or per-node to manage cluster resource usage.', Kopia
        parallel upload settings are now configurable to improve fs-backup and CSI
        data-mover throughput., 'Restore can optionally write sparse files for fs-restore
        and CSI snapshot data movement restores, which can reduce disk usage and speed
        up restores depending on workload.', "`velero backup describe` now includes\
        \ a new \u201CBackup Volumes\u201D section summarizing volume info across\
        \ native snapshots, fs-backups, CSI snapshots, and CSI snapshot data movements;\
        \ it also shows CSI info regardless of the EnableCSI client-side feature gate.",
      'Backups now include a new VolumeInfo metadata file in the repository to record
        PVC/PV, method, snapshot details, and status; this metadata is used to determine
        PV restore behavior and can be consumed by downstream tools.', 'Improved resilience
        for CSI snapshot data movement when the Velero server pod or node-agent restarts,
        reducing risk of stuck/interrupted operations.', Backup/Restore hook execution
        details are now surfaced via CR status fields (HooksAttempted/HooksFailed)
        and shown in describe output., 'AWS SDK for Go upgraded to v2, improving CPU/memory
        efficiency for AWS operations.', 'Azure AD/Workload Identity support expanded
        to cover Kopia operations (filesystem backup/data mover, etc.) in addition
        to prior Azure plugin capabilities.', 'Runtime/dependency updates: Go 1.21.6
        and Kopia 0.15.0 plus library bumps to address CVEs and keep current.']
    breaking_changes: ["`velero backup describe` output format changed: some existing\
        \ per-backup-type information moved under the new \u201CBackup Volumes\u201D\
        \ section, so scripts/parsers may break.", 'API type change: `DataMoverConfig`
        in `DataUploadSpec` changed from `*map[string]string` to `map[string]string`,
        potentially impacting custom code/controllers consuming the CRD types.', '`velero
        install` now enables informer cache by default (previously disabled), which
        can increase memory usage and may require raising pod memory limits or explicitly
        disabling it with `--disable-informer-cache`.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm values / install-time behavior to review\n- **Default\
      \ filesystem uploader changed**: `uploader-type` default is now **`kopia`**\
      \ (was `restic`). If you rely on Restic behavior/performance/tooling, set the\
      \ uploader explicitly to `restic` during/after upgrade.\n- **Helm chart config\
      \ format change (chart v4.0.0+)**: **BackupStorageLocation (BSL) and VolumeSnapshotLocation\
      \ (VSL) values changed from a map to a slice** to support multiple BSL/VSL.\
      \ This is **not backward compatible**\u2014convert your existing `configuration.backupStorageLocation`\
      \ / `configuration.volumeSnapshotLocation` values to list form **before upgrading**.\n\
      - **CSI snapshot timing settings changed**:\n  - Snapshot handle creation sync\
      \ wait is now driven by `backup.spec.csiSnapshotTimeout` (replaces fixed 10m).\n\
      \  - Async wait for `VolumeSnapshot/VolumeSnapshotContent` to become `ReadyToUse`\
      \ uses the operation timeout (default **4h**). Ensure your timeouts align with\
      \ your storage behavior.\n- **Namespace deletion behavior**: new finalizers\
      \ on Velero CRs (Restore/DataUpload/DataDownload) can cause `kubectl delete\
      \ ns velero` to hang. Prefer `velero uninstall` or remove finalizers safely."
    chart_updates: [Velero v1.12 introduces restore cleanup finalizer and new DataUpload/DataDownload
        CRDs for data mover flows; expect new CRDs and controllers in the deployment.,
      Velero moves further toward async operations and adds metrics around data upload/download
        and restore terminal timestamps; monitoring dashboards/alerts may need updates.]
    features: ['CSI Snapshot Data Movement: move CSI snapshot data into durable object
        storage and restore across environments/clouds.', 'Resource Modifiers (JSON
        substitutions): apply targeted JSON patches to resources during restore without
        writing custom RestoreItemAction plugins.', 'Multiple VolumeSnapshotClasses
        support in the CSI plugin: choose a specific VolumeSnapshotClass per backup
        instead of relying on a single labeled default.', 'Restore finalizer: `velero
        restore delete` now cleans up associated data in backup storage, not just
        the Kubernetes Restore CR.']
    breaking_changes: ['Default uploader is now Kopia instead of Restic; this can
        change backup repositories, performance characteristics, and credential/IAM
        expectations.', Helm chart v4+ changes BSL/VSL configuration format from map
        to slice; existing values files will fail or behave unexpectedly until migrated.,
      Finalizers on Velero CRs can block namespace deletion if controllers/pods are
        removed first; uninstall/delete procedures must change to avoid stuck namespaces.,
      'CSI snapshot timing/timeout behavior changed; previously fixed waits are now
        configurable/time-bound, which may surface timeouts in slow snapshot environments.']
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
