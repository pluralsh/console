icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup into a micro-service architecture, enabling concurrency
        control, cancel/resume, and resilience to node-agent restarts with steadier
        node-agent resource usage.', 'fs-backup now supports Windows workloads end-to-end
        by running data mover pods on Windows nodes, complementing the Windows CSI
        snapshot data movement added in 1.16.', 'Added Volume Group Snapshot support
        (Kubernetes beta) for CSI snapshot backups and CSI snapshot data movement
        to get point-in-time, write-order-consistent snapshots across multiple related
        volumes.', 'Added PriorityClass support so you can set pod priority separately
        for velero server, node-agent, data mover pods, and backup repository maintenance
        jobs.', 'Improved data mover scalability/resiliency: configurable node-agent
        PrepareQueueLength to throttle pending data mover pod creation, plus better
        restart handling (resume/cancel orphaned movements) and node-selection controls
        (including per storage class) for CSI snapshot data movement restores.', Extended
        resource policies with reusable include/exclude filters via includeExcludePolicy
        (in addition to volumePolicy).]
    breaking_changes: ['Restic uploader path removed in 1.17: --uploader-type=restic
        is no longer valid for new installs/backups; restores from existing restic-path
        backups remain supported only until v1.19.', 'Repository maintenance job settings
        were removed from velero server flags and must be configured via the repository
        maintenance job ConfigMap instead (removed flags: --keep-latest-maintenance-jobs,
        --maintenance-job-{cpu,mem}-{request,limit}).']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm values / install-time config changes to check\n\n### 1)\
      \ **Restic uploader removed (breaking)**\n* `--uploader-type=restic` is **no\
      \ longer valid** in v1.17.\n  * If your Helm chart values set anything equivalent\
      \ (commonly `configuration.uploaderType`, `uploaderType`, `deployRestic`, `deployNodeAgent`/`restic`\
      \ toggles, etc.), remove/disable Restic and use the current fs-backup path.\n\
      \  * You can **still restore old Restic-based backups until v1.19**, but you\
      \ cannot create new ones.\n\n### 2) **Repository maintenance job flags removed\
      \ from velero server (breaking)**\nThe following server parameters were removed\
      \ and must be configured via the **repository maintenance job ConfigMap** instead:\n\
      * `--keep-latest-maintenance-jobs`\n* `--maintenance-job-cpu-request`\n* `--maintenance-job-mem-request`\n\
      * `--maintenance-job-cpu-limit`\n* `--maintenance-job-mem-limit`\n\nIf you previously\
      \ set these via chart values that map to `server.extraArgs`, move them to the\
      \ chart\u2019s maintenance-job configmap values (name varies by chart).\n\n\
      ### 3) New/updated **node-agent** configuration knobs worth reviewing\nThese\
      \ are additive but may require Helm values if you want to use them:\n* `PrepareQueueLength`\
      \ (node-agent): throttles creation of data mover pods to avoid large numbers\
      \ stuck Pending.\n* `priorityClassName` support across modules (server, node-agent,\
      \ data mover pods, maintenance jobs): you may want to set these explicitly.\n\
      * Parameterized kubelet mount path for node-agent install (only if you run non-standard\
      \ kubelet paths).\n\n### 4) Modernized fs-backup (architecture change)\nfs-backup\
      \ is now micro-service based. In Helm terms this may translate into:\n* more/changed\
      \ pod templates for data mover / fs-backup components,\n* potential new configmaps/args\
      \ for concurrency/cancel/resume behavior.\n\n(Exact value keys depend on your\
      \ chart; validate rendered manifests before applying.)"
    chart_updates: ['fs-backup moved to a micro-service architecture (better concurrency
        control, cancel/resume, and resiliency across node-agent restarts).', 'Windows
        support expands: fs-backup now supports Windows workloads; data mover pods
        can run on Windows nodes with required tolerations.', Volume Group Snapshots
        support added (Kubernetes beta feature) for CSI snapshot backup and CSI snapshot
        data movement., 'PriorityClass support added across Velero components (server,
        node-agent, data mover pods, maintenance jobs).', 'Data mover scalability
        improvements: throttled pod creation via node-agent `PrepareQueueLength`,
        improved restart/orphan handling, expanded node-selection for restore and
        per-storageclass node-selection.', Resource policy enhanced with reusable
        include/exclude filters via `includeExcludePolicy`., 'Operational changes:
        repository maintenance job configuration moved from server flags to a ConfigMap;
        additional config validation added for install CLI and server start.']
    features: [Modernized fs-backup into a micro-service architecture with concurrency
        control plus cancel/resume and improved resiliency across node-agent restarts.,
      'fs-backup now supports Windows workloads, enabling full Windows backup/restore
        scenarios (with CSI data movement support introduced earlier).', Adds support
        for Kubernetes Volume Group Snapshots to take point-in-time consistent snapshots
        across multiple related volumes., 'Adds PriorityClass support so you can control
        scheduling priority for server, node-agent, data movers, and maintenance jobs.',
      'Improves data mover scalability with a node-agent prepare queue to avoid flooding
        the cluster with Pending pods, plus better restart/orphan handling and node-selection
        (including per-storageclass).', Adds reusable resource include/exclude filtering
        via `includeExcludePolicy` in resource policies.]
    breaking_changes: ["Restic uploader path is removed: `--uploader-type=restic`\
        \ is no longer a valid install configuration; you can\u2019t create new Restic-based\
        \ backups (restores remain supported until v1.19).", Repository maintenance
        job settings are no longer configured via Velero server flags; the maintenance
        job CPU/memory and keep-latest settings must be moved to the repository maintenance
        job ConfigMap., 'PVC restore behavior change: selected-node annotation is
        now removed during PVC restore when no node mapping exists (previously it
        could be preserved in some cases).']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm values / install-time flags to review (v1.15 \u279C v1.16)\n\
      \n> You didn\u2019t include Helm chart release notes here, so this is **inferred\
      \ from app flags/config** mentioned in the Velero release notes. Verify exact\
      \ Helm value keys against your chart version.\n\n### 1) New server flag: parallel\
      \ ItemBlock backups\n- **New flag:** `--item-block-worker-count` (default `1`).\n\
      - **What to do:** If you want faster Kubernetes-object backups, set this >1\
      \ (start small, e.g. 2\u20135) and monitor API server load and Velero CPU/memory.\n\
      \n### 2) Node-agent config: data mover restore scheduling for WaitForFirstConsumer\
      \ volumes\n- **New node-agent config flag:** `ignoreDelayBinding`.\n- **What\
      \ to do:** If you use CSI data movement and have `WaitForFirstConsumer` PVCs,\
      \ consider enabling this to increase restore parallelism / better node spreading.\n\
      \n### 3) BackupRepository config: kopia maintenance interval control\n- **New\
      \ option:** `fullMaintenanceInterval` with modes like `normalGC`, `fastGC`,\
      \ `eagerGC`.\n- **What to do:** If repo maintenance and deletion lag are concerns,\
      \ tune this; faster GC may increase maintenance frequency/resource usage.\n\n\
      ### 4) Image change: restore helper merged into main Velero image\n- **Change:**\
      \ `velero-restore-helper` is now included in the single `velero/velero` image.\n\
      - **What to do:** If your Helm values reference a separate restore-helper image,\
      \ that may become unnecessary/unsupported; confirm chart behavior.\n\n### 5)\
      \ Windows/hybrid clusters\n- **What to do:** If you run hybrid Linux/Windows,\
      \ ensure node-agent/data mover tolerations, node selectors, and security context\
      \ settings align with your cluster policy. Also note fs-backup limitations on\
      \ Windows.\n"
    chart_updates: []
    features: [Windows cluster support for running Velero components and backing up/restoring
        Windows workloads via the built-in data mover (with documented limitations).,
      Parallel ItemBlock backup processing to improve backup throughput; controlled
        by `--item-block-worker-count`., Data mover restore scalability improvement
        for `WaitForFirstConsumer` volumes via node-agent `ignoreDelayBinding` to
        spread restore work across nodes., 'Improved data mover observability: more
        intermediate object statuses and cleanup error logging in node-agent logs.',
      'CSI snapshot usability improvement: stop retaining/syncing unnecessary `VolumeSnapshotContent`
        objects in backups/restores.', 'BackupRepository maintenance resiliency/observability:
        `RecentMaintenance` history in CR status and better behavior across server
        restarts and read-only BSLs.', Volume Policy can now filter volumes by PVC
        labels., Per-object control of restoring resource status using the `velero.io/restore-status`
        annotation., Velero binaries (including restore helper) consolidated into
        a single Velero image.]
    breaking_changes: [No explicit breaking changes are called out in the v1.16.0
        release notes you provided; main changes are additive. Still validate any
        custom automation that assumes a separate restore-helper image or relies on
        prior CSI snapshot artifacts being present in backups.]
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['**Data mover micro service for CSI snapshot data movement:** Data
        transfer work moves from node-agent pods into dedicated per-backup/per-restore
        pods, improving isolation, resiliency, and allowing per-operation CPU/memory
        tuning while avoiding privileged hostPath access.', '**Backup PVC configuration
        for data movement:** New configMap allows making backup PVC mounts read-only
        (faster exposes for some storage like Ceph) and choosing a different StorageClass
        for backup PVCs than workload PVCs.', '**Repository maintenance scheduling
        improvements:** New configMap enables node selection (and resource config
        via configMap) for repository maintenance jobs so you can steer these heavy
        jobs away from critical nodes.', '**Backup repository cache limit:** New configMap
        lets you cap client-side repository cache size per repo to avoid filling pod
        root filesystem/ephemeral storage and getting evicted.', '**Backup performance
        groundwork (Item Blocks / IBA):** Introduces Item Block concepts and ItemBlockAction
        plugin type to categorize resources for future multi-threaded backups; built-in
        IBAs for Pods and PVCs are included (actual multi-thread processing comes
        later).', '**Performance and stability improvements:** Fixes a post-plugin-call
        memory leak, passes client-qps/burst settings through to plugins, and includes
        Kopia upstream improvements that reduce high-memory maintenance scenarios.']
    breaking_changes: ["**Restic deprecation begins (fs-backup):** Restic uploader\
        \ path is deprecated starting 1.15; backups/restores still work but emit warnings\
        \ when restic is configured/used\u2014plan to migrate to kopia.", '**node-agent
        configMap name is no longer fixed:** You must set/verify the `--node-agent-configmap`
        server parameter if you rely on a non-default name; Velero now supports customizing
        it.', '**Repository maintenance job flags moved to configMap (preferred):**
        Maintenance job CPU/mem request/limits and job retention flags are now primarily
        configured via the new repository maintenance job configuration configMap;
        CLI flags remain for backward compatibility but configMap values take precedence.',
      '**Changing PVC selected-node restore feature deprecated:** This restore behavior
        is now deprecated and may be removed in a future release; avoid relying on
        it.']
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: [Repository maintenance for Kopia/Restic now runs in separate Kubernetes
        Jobs to reduce Velero server pod memory pressure (helps avoid OOM during repo
        connect/maintenance)., 'VolumePolicies now support actions (e.g., force `fs-backup`
        vs `snapshot`) for finer-grained per-volume backup behavior without changing
        workloads.', Data movement backups can target eligible nodes via a ConfigMap
        to control where datamover pods run (resource/placement control)., 'Restore
        flow now persists VolumeInfo metadata similar to backups, and `velero restore
        describe` shows more CSI snapshot/data-movement details.', Restores gain a
        new `Finalizing` phase to fix ordering issues (PV patching after volume restore;
        post-restore hooks after data movement completes)., 'Azure authentication
        adds support for service principals using certificate-based auth (phishing-resistant,
        aligns with Azure recommendations).']
    breaking_changes: [CSI plugin is now merged into the Velero repo and installed
        by default as an internal plugin; do not install it via `--plugins` during
        `velero install` anymore., 'Default resource requests/limits for node-agent
        are removed, making node-agent pods BestEffort by default; you must set resources
        explicitly if you need guaranteed QoS.', 'Backup namespace filtering changes
        when `includedNamespaces`/`excludedNamespaces` are empty but label selectors
        are set: only namespaces containing matching resources are backed up (previously
        all namespaces were included).', PV patching in the new Restore `Finalizing`
        phase can cause restores to end `PartiallyFailed` if PV remains `Pending`;
        previously these might appear `Complete`., 'API/runtime upgrades may impact
        custom builds/plugins: Go runtime moves to 1.22.2 and Kopia to 0.17.0.']
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ['Application upgrade: Velero v1.12.0 -> v1.13.0 (new server/node-agent
        behavior and new CLI output formats).', 'Defaults/behavior changes to be aware
        of during install/upgrade: informer cache is now enabled by default, which
        can increase Velero pod memory usage; can be disabled if needed.', New backup
        repository metadata file (VolumeInfo) is now written with backups and used
        during PV restore decisions; compatibility fallback exists for older backups
        but is planned to be removed in v1.15., New CRD status fields for hook execution
        details are now populated and shown in `velero backup/restore describe`.,
      'API type change in v2alpha1 DataUploadSpec: `DataMoverConfig` changes from
        `*map[string][string]` to `map[string]string` (client/code generation impacts
        for anyone interacting with the CRDs programmatically).']
    features: ['Resource Modifiers now support JSON Merge Patch and Strategic Merge
        Patch in addition to JSON Patch, enabling more flexible restore-time resource
        changes.', Node-agent concurrency controls let you cap data-movement workloads
        per node (globally or by-node) for better cluster resource management., Kopia
        uploader parallel file upload options can speed up fs-backups and CSI snapshot
        data movement by increasing concurrency during upload., 'Sparse file writing
        during restore is supported for fs-restore and CSI snapshot data movement
        restores, potentially improving performance/storage efficiency.', '`velero
        backup describe` now includes a dedicated Backup Volumes section and can show
        CSI snapshot data movement details regardless of client-side EnableCSI feature
        gate.', Backups now include a VolumeInfo metadata file with PVC/PV and snapshot/method/status
        details to drive PV restore decisions and aid downstream tooling., Backup/restore
        hook execution status is now surfaced in CR status and `describe` output (attempted/failed
        counts)., 'AWS SDK for Go is bumped to v2, improving CPU and memory efficiency
        for AWS interactions.', 'Azure AD/Workload Identity is now supported for Kopia
        operations (filesystem backup/data mover) in Azure scenarios, expanding beyond
        native snapshots.', 'Runtime/dependency refresh: Go 1.21.6 and Kopia 0.15.0
        plus library bumps for CVE fixes.']
    breaking_changes: ['`velero backup describe` output format changed: some existing
        snapshot/fs-backup info moved into the new Backup Volumes section (scripts/parsers
        may break).', 'API type change for DataUpload v2alpha1: `DataUploadSpec.DataMoverConfig`
        is no longer a pointer; any automation applying/patching this field must adapt.',
      '`velero install` behavior change: informer cache is enabled by default (previously
        disabled), which may require higher memory limits or explicit disablement
        to avoid OOM.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm chart changes to plan for (Velero chart v4.0.0+)\n- **`configuration.backupStorageLocation`\
      \ (BSL) and `configuration.volumeSnapshotLocation` (VSL) changed type: `map`\
      \ \u2192 `list` (slice).**\n  - This is **not backward compatible**; update\
      \ your `values.yaml` *before* upgrading.\n  - Typical pattern:\n    - **Before\
      \ (v1.11 / older chart):**\n      ```yaml\n      configuration:\n        backupStorageLocation:\n\
      \          name: default\n          provider: aws\n          bucket: my-bucket\n\
      \      ```\n    - **After (chart v4+):**\n      ```yaml\n      configuration:\n\
      \        backupStorageLocation:\n          - name: default\n            provider:\
      \ aws\n            bucket: my-bucket\n      ```\n      (same idea for `volumeSnapshotLocation`)\n\
      - If you deploy plugins via chart values, ensure plugin image tags are compatible\
      \ with Velero **v1.12.0** (especially if you adopt Kopia / data mover features)."
    chart_updates: [Helm chart v4.0.0 introduces support for **multiple** BackupStorageLocations
        and VolumeSnapshotLocations., "BSL/VSL values schema change (map\u2192list)\
        \ required to support the multi-location capability; upgrade is not backward\
        \ compatible."]
    features: ['CSI Snapshot Data Movement: move CSI snapshot data into durable backup
        storage and restore to same or different environments/clouds.', 'Resource
        Modifiers (JSON substitutions): apply JSON patch-style modifications to selected
        resources during restore without writing a custom RestoreItemAction plugin.',
      'Multiple VolumeSnapshotClasses support in the CSI plugin: choose a specific
        VolumeSnapshotClass per backup instead of relying on a single labeled class.',
      'Restore finalizer cleanup: `velero restore delete` now also cleans up associated
        external data in the backup storage location.', 'Runtime/deps updates: Golang
        bumped to 1.20.7 and Kopia bumped to 0.13.x with assorted dependency updates.']
    breaking_changes: [Default uploader type changed from **restic** to **kopia**;
        filesystem backups will use Kopia by default unless you explicitly set `uploader-type`
        back to restic., 'CSI snapshot timing/timeout behavior changed: snapshot sync
        wait is now configurable via `backup.spec.csiSnapshotTimeout`, and async readiness
        waits use operation timeouts (default 4 hours).', Helm chart v4.0.0+ changes
        BSL/VSL configuration from map to list (slice); you must convert existing
        values prior to upgrade., 'Finalizers on Velero CRs (e.g., restore/dataupload/datadownload)
        can cause **namespace deletion to hang**; use `velero uninstall` or remove
        finalizers properly instead of deleting the namespace directly.']
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
