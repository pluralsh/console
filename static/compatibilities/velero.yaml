icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup to a micro-service architecture, adding concurrency
        control, cancel/resume, and better resiliency/resource isolation for backups/restores.',
      'fs-backup now supports Windows workloads by running data mover pods on Windows
        nodes, complementing prior CSI snapshot data movement Windows support.', Added
        support for Kubernetes VolumeGroupSnapshot (beta upstream) for both CSI snapshot
        backups and CSI snapshot data movement to improve write-order consistency
        across multiple volumes., 'PriorityClass can now be configured across Velero
        components (server, node-agent, data mover pods, and repository maintenance
        jobs) to influence scheduling and eviction behavior.', 'Scalability/resiliency
        improvements for data movers: configurable prepare-queue length to limit pending
        pod buildup; data movements can resume after node-agent restarts and be cancelled
        if orphaned; and restore-side node selection including per-storage-class selection.',
      Resource policy configmaps now support reusable include/exclude filtering via
        includeExcludePolicy in addition to volumePolicy., 'Operational improvements:
        BSL availability metric and better BSL/repository readiness handling on restart;
        imagePullSecrets inheritance for data mover/maintenance jobs; improved handling
        of hooks and restore init containers; added s390x build support.']
    breaking_changes: ['Restic-based filesystem backups are removed: --uploader-type=restic
        is no longer a valid install configuration (restores from existing restic
        backups are supported until v1.19).', 'Repository maintenance job settings
        were removed from velero server flags (--keep-latest-maintenance-jobs, --maintenance-job-*-request/limit);
        these settings now live in a repository maintenance job ConfigMap.', 'PVC
        restore behavior change: selected-node annotation is always removed when no
        node mapping exists (previously preserved if the node existed).']
  chart_version: 11.3.1
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm values / install-time config changes to check\n\n### 1)\
      \ **Restic uploader removed (breaking)**\n* `--uploader-type=restic` is **no\
      \ longer valid** in v1.17.\n  * If your Helm chart values set anything equivalent\
      \ (commonly `configuration.uploaderType`, `uploaderType`, `deployRestic`, `deployNodeAgent`/`restic`\
      \ toggles, etc.), remove/disable Restic and use the current fs-backup path.\n\
      \  * You can **still restore old Restic-based backups until v1.19**, but you\
      \ cannot create new ones.\n\n### 2) **Repository maintenance job flags removed\
      \ from velero server (breaking)**\nThe following server parameters were removed\
      \ and must be configured via the **repository maintenance job ConfigMap** instead:\n\
      * `--keep-latest-maintenance-jobs`\n* `--maintenance-job-cpu-request`\n* `--maintenance-job-mem-request`\n\
      * `--maintenance-job-cpu-limit`\n* `--maintenance-job-mem-limit`\n\nIf you previously\
      \ set these via chart values that map to `server.extraArgs`, move them to the\
      \ chart\u2019s maintenance-job configmap values (name varies by chart).\n\n\
      ### 3) New/updated **node-agent** configuration knobs worth reviewing\nThese\
      \ are additive but may require Helm values if you want to use them:\n* `PrepareQueueLength`\
      \ (node-agent): throttles creation of data mover pods to avoid large numbers\
      \ stuck Pending.\n* `priorityClassName` support across modules (server, node-agent,\
      \ data mover pods, maintenance jobs): you may want to set these explicitly.\n\
      * Parameterized kubelet mount path for node-agent install (only if you run non-standard\
      \ kubelet paths).\n\n### 4) Modernized fs-backup (architecture change)\nfs-backup\
      \ is now micro-service based. In Helm terms this may translate into:\n* more/changed\
      \ pod templates for data mover / fs-backup components,\n* potential new configmaps/args\
      \ for concurrency/cancel/resume behavior.\n\n(Exact value keys depend on your\
      \ chart; validate rendered manifests before applying.)"
    chart_updates: ['fs-backup moved to a micro-service architecture (better concurrency
        control, cancel/resume, and resiliency across node-agent restarts).', 'Windows
        support expands: fs-backup now supports Windows workloads; data mover pods
        can run on Windows nodes with required tolerations.', Volume Group Snapshots
        support added (Kubernetes beta feature) for CSI snapshot backup and CSI snapshot
        data movement., 'PriorityClass support added across Velero components (server,
        node-agent, data mover pods, maintenance jobs).', 'Data mover scalability
        improvements: throttled pod creation via node-agent `PrepareQueueLength`,
        improved restart/orphan handling, expanded node-selection for restore and
        per-storageclass node-selection.', Resource policy enhanced with reusable
        include/exclude filters via `includeExcludePolicy`., 'Operational changes:
        repository maintenance job configuration moved from server flags to a ConfigMap;
        additional config validation added for install CLI and server start.']
    features: [Modernized fs-backup into a micro-service architecture with concurrency
        control plus cancel/resume and improved resiliency across node-agent restarts.,
      'fs-backup now supports Windows workloads, enabling full Windows backup/restore
        scenarios (with CSI data movement support introduced earlier).', Adds support
        for Kubernetes Volume Group Snapshots to take point-in-time consistent snapshots
        across multiple related volumes., 'Adds PriorityClass support so you can control
        scheduling priority for server, node-agent, data movers, and maintenance jobs.',
      'Improves data mover scalability with a node-agent prepare queue to avoid flooding
        the cluster with Pending pods, plus better restart/orphan handling and node-selection
        (including per-storageclass).', Adds reusable resource include/exclude filtering
        via `includeExcludePolicy` in resource policies.]
    breaking_changes: ["Restic uploader path is removed: `--uploader-type=restic`\
        \ is no longer a valid install configuration; you can\u2019t create new Restic-based\
        \ backups (restores remain supported until v1.19).", Repository maintenance
        job settings are no longer configured via Velero server flags; the maintenance
        job CPU/memory and keep-latest settings must be moved to the repository maintenance
        job ConfigMap., 'PVC restore behavior change: selected-node annotation is
        now removed during PVC restore when no node mapping exists (previously it
        could be preserved in some cases).']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Windows cluster support: Velero can run in hybrid Linux/Windows clusters
        and back up/restore Windows workloads using a hybrid (multi-arch + Windows)
        all-in-one image; node-agent/data mover/maintenance jobs can be scheduled
        to Windows nodes where applicable.', 'Parallel ItemBlock backup: item blocks
        (and their pre/post hooks) can now be processed concurrently to improve backup
        throughput; parallelism is controlled by the new server flag `--item-block-worker-count`
        (default 1).', 'Data mover restore scalability: for WaitForFirstConsumer volumes,
        restores can be distributed across nodes instead of being constrained to the
        attached node by setting `ignoreDelayBinding` in the node-agent config.',
      'Data mover observability: node-agent logs now include richer diagnostics for
        intermediate objects and cleanup failures during data mover backup/restore,
        enabled by default.', 'CSI snapshot usability: removes unnecessary retained
        `VolumeSnapshotContent` from backups to avoid syncing/restoring extra CSI
        objects across clusters sharing a BSL.', 'Backup repository maintenance improvements:
        adds `RecentMaintenance` history to `BackupRepository` CRs, recaptures running
        maintenance jobs after server restarts, avoids maintenance/initialization
        on readOnly BSLs, and supports configurable `fullMaintenanceInterval` (normalGC/fastGC/eagerGC)
        via backup-repository config.', 'Volume Policy enhancement: can filter volumes
        by PVC labels, improving selection control for backups.', 'Per-object resource
        status restore: opt in/out of status restoration per object via `velero.io/restore-status`
        annotation.', 'Restore helper consolidation: `velero`, `velero-helper`, and
        `velero-restore-helper` binaries are bundled into the single Velero image,
        reducing image sprawl.']
    breaking_changes: ['No explicit breaking changes called out in the provided v1.16.0
        notes; however, Windows support has functional limitations (no fs-backup for
        Windows, no security descriptors/NTFS xattrs), and new parallel backup behavior
        may increase API/server load unless `--item-block-worker-count` is tuned.']
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ["No Helm chart changelog was provided in the notes you shared,\
        \ so chart-level changes (values keys, defaults, templates, manifests) can\u2019\
        t be summarized from this input.", "Treat the items below as **Velero application**\
        \ changes; verify the corresponding Helm chart version\u2019s CHANGELOG/values.yaml\
        \ before upgrading."]
    features: ['Data mover micro service: CSI snapshot data movement now runs in dedicated
        backup/restore pods rather than node-agent pods, improving security (less
        hostPath), per-job resource control, and isolation.', Item Block concept +
        ItemBlockAction (IBA) plugin type added to group correlated resources for
        future multi-threaded backups; built-in IBAs for Pods and PVCs included (multi-threaded
        processing not enabled yet)., 'Repository maintenance job node selection:
        new configMap to control where maintenance jobs run so you can target idle
        nodes or avoid critical nodes.', 'Backup PVC configuration: options to mount
        backupPVCs read-only (can speed up expose for some storage like Ceph) and
        to choose a different storageClass for backupPVCs.', 'Backup repository cache
        configuration: per-repository cache size limit via new configMap to reduce
        risk of eviction due to ephemeral storage pressure.', 'Performance improvements:
        fixes memory leak after plugin calls; plugin processes inherit client QPS/burst;
        kopia upstream improvements reduce high memory during maintenance in large-file
        repositories.']
    breaking_changes: ['Restic deprecation begins: restic uploader path still works
        in 1.15 but emits warnings (e.g., when installed with --uploader-type=restic
        or when restic path is used for fs-backup/restore); plan migration to kopia.',
      Node-agent configMap name is no longer fixed; you must pass the server flag
        `--node-agent-configmap` if you use a non-default configMap name (and chart
        values may need to set this)., 'Repository maintenance job tuning flags are
        moving from server args to a new repository maintenance configMap; existing
        flags still work for backward compatibility, but configMap values take precedence
        if both are set.', Changing PVC selected-node feature is deprecated and scheduled
        for removal in a future release; avoid relying on it going forward.]
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: [Repository maintenance for kopia/restic backup repositories is moved
        out of the Velero server pod into separate Kubernetes Jobs to avoid Velero
        pod OOMs; install can now set resource requests for these jobs., 'VolumePolicies
        now support explicit volume backup actions (e.g., force a volume to use `fs-backup`
        or `snapshot`), enabling opt-in/opt-out style control without modifying workloads.',
      'Data mover pods (for snapshot data movement/fs backups) can be scheduled only
        onto a selectable set of nodes via a ConfigMap, to control resource placement.',
      'Restore operations now persist VolumeInfo metadata similar to backups, and
        `velero restore describe` shows richer volume/CSI restore info.', Restores
        gain a new `Finalizing` phase so PV label patching and post-restore hooks
        occur after underlying data restore/data movement completes., Azure authentication
        gains support for service principal certificate-based auth (phishing-resistant)
        in addition to secret-based auth.]
    breaking_changes: [CSI plugin is now merged into the Velero repo and installed
        by default as an internal plugin; do not install it via `velero install --plugins
        ...` anymore., Default resource requests/limits for the node-agent are removed
        so node-agent pods become QoS `BestEffort`; you may need to explicitly set
        requests/limits if you relied on the defaults., 'Namespace filtering during
        backup changes when `includedNamespaces`/`excludedNamespaces` are unset but
        label selectors are set: only namespaces containing matching resources are
        included (previously all namespaces were included).', 'PV patching during
        the new `Finalizing` state can make restores end `PartiallyFailed` when PVs
        are stuck `Pending`, where older versions might report `Complete`.']
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm/chart-related notes to carry into the 1.12 \u2192 1.13\
      \ upgrade\n\n- **(From 1.12 notes but still relevant if you haven\u2019t already\
      \ done it): BSL/VSL values format change in Helm chart v4.0.0**\n  - `configuration.backupStorageLocation`\
      \ and `configuration.volumeSnapshotLocation` changed **from map \u2192 list/slice**.\n\
      \  - This is **not backward compatible**; convert values to lists **before**\
      \ upgrading.\n\n- **Velero install flag change that can impact Helm-based installs:**\n\
      \  - v1.13 enables the **informer cache by default** (equivalent to `--disable-informer-cache=false`).\n\
      \  - If you see OOMs after upgrade, increase Velero server memory limits or\
      \ set the install option to disable the cache (`--disable-informer-cache`).\
      \ In Helm, this typically maps to adding the arg in `configuration.extraArgs`/`initContainers`/`deploy`\
      \ args depending on your chart values layout.\n\n- **New/updated CLI/install\
      \ knobs you may want to reflect in Helm values:**\n  - Node-agent concurrency\
      \ configuration (new config key; see docs) to limit per-node data-movement load.\n\
      \  - Parallel upload and sparse file restore options (used by backups/restores,\
      \ not always Helm values, but may be exposed as server/node-agent args in some\
      \ deployments)."
    chart_updates: []
    features: ['Resource Modifiers enhanced: in addition to JSON Patch, v1.13 adds
        JSON Merge Patch and Strategic Merge Patch for more flexible restore-time
        mutations.', 'Node-agent concurrency controls: you can cap how many fs-backup
        / CSI snapshot data-mover activities run per node (globally or per node) to
        manage CPU/memory/network usage.', 'Kopia performance features: configurable
        parallel file upload options to speed up fs-backups and CSI snapshot data
        movement.', 'Sparse file restore option: restores can write sparse files for
        fs-restore and CSI snapshot data movement, reducing time/space for sparse
        workloads.', "Backup describe output improved: new \u201CBackup Volumes\u201D\
        \ section and better visibility into CSI snapshot data movement; client-side\
        \ CSI feature-gate checks removed for describe.", 'New VolumeInfo metadata
        stored with backups to describe PV/PVC backup method, snapshots, and status;
        used to drive PV restore logic.', Better resilience to Velero server/node-agent
        restarts during CSI snapshot data movement operations., Hook execution status
        fields added to Backup/Restore status and shown in describe output (hooks
        attempted/failed counts)., AWS SDK for Go bumped to v2 for better CPU/memory
        performance., 'Azure AD/Workload Identity support extended to Kopia operations
        (fs-backup/data mover), not just native snapshots.', 'Runtime/deps: Go 1.21.6
        and Kopia 0.15.0 bumps. ']
    breaking_changes: ["Backup describe output format changed; some existing snapshot/fs-backup\
        \ details moved under the new \u201CBackup Volumes\u201D section, so any scripts\
        \ parsing output may break.", 'API type change: `DataUploadSpec.DataMoverConfig`
        changed from `*map[string]string` to `map[string]string` (client code or manifests
        relying on nullability may need updates).', '`velero install` behavior change:
        informer cache is now enabled by default; can increase memory usage and may
        require tuning resource limits or explicitly disabling the cache.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm chart / values changes to address before upgrading\n-\
      \ **Velero Helm chart v4.0.0+ (used for Velero v1.12)** supports **multiple\
      \ BackupStorageLocations (BSL) and VolumeSnapshotLocations (VSL)**.\n  - **Breaking:**\
      \ `configuration.backupStorageLocation` and `configuration.volumeSnapshotLocation`\
      \ **changed from a map/object to a list/slice**.\n  - **Action:** convert existing\
      \ single BSL/VSL values into list form **before** the upgrade (or in the same\
      \ PR) to avoid a failed/incorrect render.\n  - Ref: chart release `velero-4.0.0`\
      \ / PR #413 (as called out in the Velero v1.12 release notes)."
    chart_updates: [Chart v4.0.0+ supports multiple BSL and VSL; configuration structure
        changed from map to slice (not backwards compatible).]
    features: ['CSI Snapshot Data Movement: ability to move CSI snapshot data from
        ephemeral/limited snapshot systems into durable backup storage and restore
        across environments/clouds.', 'Resource Modifiers (JSON substitutions): apply
        JSONPatch-like modifications to selected resources during restore without
        writing custom RestoreItemAction plugins.', 'Multiple VolumeSnapshotClasses
        support in the CSI plugin: choose a specific VolumeSnapshotClass per backup
        instead of relying on a single labeled class.', 'Restore finalizer: `velero
        restore delete` now cleans up associated restore data in the backup storage
        location, not just the Kubernetes Restore CR.']
    breaking_changes: ['Default `uploader-type` changed from `restic` to `kopia`,
        so filesystem backups will use Kopia unless explicitly set otherwise; validate
        behavior, credentials, and known limitations before upgrading.', 'CSI snapshot
        timing/timeout behavior changed: snapshot handle sync wait is now controlled
        by `backup.spec.csiSnapshotTimeout` (instead of fixed 10 minutes) and async
        ReadyToUse waits use the operation timeout (default 4 hours).', Helm chart
        v4.0.0+ changes BSL/VSL values from map to slice (not backwards compatible);
        update values prior to upgrade., 'Finalizers added to Velero CRs (e.g., restore,
        dataupload, datadownload) can cause `kubectl delete namespace <velero-ns>`
        to hang; use `velero uninstall` or remove finalizers safely instead of deleting
        the namespace directly.']
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
