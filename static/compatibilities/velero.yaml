icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm/values changes to account for in the upgrade\n\n> Note:\
      \ you provided *application* release notes (Velero v1.17.0), not the Helm chart\
      \ changelog. The items below are the most likely Helm **values/args** impacts\
      \ based on the Velero server/node-agent flags and config behavior described\
      \ in the release notes.\n\n- **Remove any Restic uploader configuration**\n\
      \  - If your Helm values set anything equivalent to `--uploader-type=restic`\
      \ (or a chart value that selects `restic`), **remove it**. Restic as an uploader\
      \ type is no longer valid for new backups in v1.17.\n  - You may keep legacy\
      \ restores from existing Restic-based backups until v1.19.\n\n- **Repository\
      \ maintenance job settings moved to ConfigMap (server flags removed)**\n  -\
      \ If your Helm chart previously set server args:\n    - `--keep-latest-maintenance-jobs`\n\
      \    - `--maintenance-job-cpu-request`\n    - `--maintenance-job-mem-request`\n\
      \    - `--maintenance-job-cpu-limit`\n    - `--maintenance-job-mem-limit`\n\
      \  - \u2026then you must **migrate these to the repository maintenance job ConfigMap**\
      \ (often chart-managed). Ensure your values now render/configure that ConfigMap\
      \ instead of server args.\n  - v1.17 adds **ConfigMap parameter validation**\
      \ at install/server start; invalid keys/values will now fail fast.\n\n- **PriorityClass\
      \ support (optional new values)**\n  - v1.17 allows setting PriorityClass separately\
      \ for:\n    - velero server\n    - node-agent\n    - data mover pods\n    -\
      \ repository maintenance jobs\n  - If your chart exposes values for `priorityClassName`,\
      \ you can now/should set them consistently (especially in constrained clusters).\n\
      \n- **Node-agent/data mover scaling control (optional new config)**\n  - A node-agent\
      \ config parameter `PrepareQueueLength` can throttle creation of data mover\
      \ pods to avoid many Pending pods.\n  - If your chart manages the node-agent\
      \ ConfigMap, consider exposing/setting this for large environments.\n\n- **Windows\
      \ tolerations / scheduling (if applicable)**\n  - v1.17 adds `os=windows:NoSchedule`\
      \ toleration for Windows pods; verify your Helm values don\u2019t override tolerations/nodeSelectors\
      \ in a way that blocks Windows support.\n\n- **Kubelet mount path now parameterized\
      \ (node-agent install)**\n  - If you have non-standard kubelet paths (common\
      \ in some distros), ensure your chart values align with the new parameterization\
      \ and your existing hostPath mounts."
    chart_updates: ['No Helm chart changelog was provided, so chart-template changes
        (new/removed values, renamed keys, new manifests) cannot be stated with certainty.',
      'Operationally relevant changes from the application that commonly affect chart
        templates/config include: removal of Restic uploader type, migration of maintenance-job
        configuration to a ConfigMap, and expanded PriorityClass wiring across components.',
      'v1.17 introduces stronger ConfigMap parameter validation, which can surface
        misconfigurations in chart-rendered ConfigMaps that previously went unnoticed.']
    features: ['Modernized fs-backup to a micro-service architecture, improving robustness
        (survives node-agent restarts), adding concurrency control, and enabling cancel/resume
        behaviors.', 'fs-backup now supports Windows workloads by allowing data mover
        pods to run on Windows nodes, completing end-to-end Windows backup/restore
        scenarios.', 'Adds support for Kubernetes Volume Group Snapshots (beta upstream),
        enabling crash-consistent snapshots across multiple related volumes.', 'PriorityClass
        support across Velero modules (server, node-agent, data movers, maintenance
        jobs) for better scheduling control.', 'Data mover scalability/resiliency
        improvements: throttling pod creation via PrepareQueueLength, better restart-resume
        behavior, and improved node-selection (including per-storage-class).', 'Resource
        policy enhancement: reusable include/exclude filters (`includeExcludePolicy`)
        in resource policy ConfigMaps.', CLI can auto-discover and use CA certs from
        the BackupStorageLocation for download requests; improved BSL availability
        metrics and checks.]
    breaking_changes: ['Restic uploader path removed: `--uploader-type=restic` is
        no longer valid for installation/new backups (restores from existing Restic
        backups still supported until v1.19).', Repository maintenance job configuration
        flags were removed from the Velero server; these settings must be configured
        via the maintenance-job ConfigMap going forward., 'PVC restore behavior change:
        when no node mapping exists, Velero always removes the `selected-node` annotation
        (previously it could be preserved if the node existed), which may change scheduling
        outcomes on restore.']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup to a micro-service architecture with better concurrency
        control, cancel/resume, and resilience across node-agent restarts.', 'Added
        fs-backup support for Windows workloads, enabling full Windows backup/restore
        scenarios when combined with CSI snapshot data movement.', Added support for
        Kubernetes VolumeGroupSnapshot (beta upstream) to take point-in-time consistent
        snapshots across multiple related volumes., 'Added PriorityClass support across
        Velero components (server, node-agent, data mover pods, and maintenance jobs).',
      'Improved data mover scalability/resiliency: queueing to limit Pending pods
        (PrepareQueueLength), resume/cancel safely on node-agent restart, and restore
        node-selection including per-storage-class node selection.', Extended resource
        policies with reusable include/exclude filters (includeExcludePolicy) in addition
        to volumePolicy.]
    breaking_changes: ['Restic fs-backup path is removed in v1.17: --uploader-type=restic
        is no longer valid for installation/creating new backups; restores from existing
        Restic backups remain supported only until v1.19.', Repository maintenance
        job tuning flags were removed from velero server parameters and must be configured
        via the repository maintenance job ConfigMap (keep-latest-maintenance-jobs
        and maintenance job CPU/memory requests/limits).]
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ['Velero v1.16.0 adds first-class Windows cluster support (hybrid/multi-arch
        build, node-agent/data mover/maintenance jobs can run on Windows nodes) with
        explicit limitations (no fs-backup for Windows; no Security Descriptor/NTFS
        extended attributes backup/restore).', 'Backup performance/scalability: Item
        Blocks can now be processed in parallel via a thread pool; pre/post hooks
        run within item blocks and therefore can also run in parallel. Parallelism
        is controlled by the new server flag `--item-block-worker-count` (default
        1).', 'Data mover restore scalability: new node-agent config option `ignoreDelayBinding`
        allows distributing restores for WaitForFirstConsumer PVCs across nodes instead
        of forcing the attached node.', 'Observability improvements: node-agent logs
        now emit more intermediate-object status and cleanup failure diagnostics for
        data mover backup/restore automatically.', 'CSI snapshot usability: unnecessary
        retained `VolumeSnapshotContent` objects are no longer included in backups
        (reduces cross-cluster sync/restore noise).', 'BackupRepository maintenance
        resiliency/visibility: BackupRepository CR gains `RecentMaintenance` history;
        maintenance jobs are recaptured after server restart; maintenance/init is
        skipped for readOnly BSLs; configurable `fullMaintenanceInterval` supports
        `normalGC`/`fastGC`/`eagerGC`.', 'Volume Policy enhancements: can filter volumes
        by PVC labels; also extends PV filtering for additional CSI VolumeAttributes
        properties.', 'Restore behavior: object-level control to restore resource
        status via annotation `velero.io/restore-status`.', 'Packaging: `velero-helper`
        and `velero-restore-helper` are now bundled into the main `velero/velero`
        image (no separate restore-helper image).', 'Dependency bumps: Go runtime
        to 1.23.7; kopia to 0.19.0; Kubernetes libs (client-go/controller-runtime)
        updated, improving logging consistency.']
    features: [Windows cluster support for Velero components and data mover backup/restore
        of Windows workloads (with known limitations)., Parallel Item Block backups
        to improve throughput; configurable via `--item-block-worker-count`., Data
        mover restore can be distributed across nodes for WaitForFirstConsumer PVCs
        using node-agent `ignoreDelayBinding`., Improved data mover observability
        with additional intermediate-object status and cleanup error logging in node-agent
        logs., CSI snapshot backup/restore now omits retained VolumeSnapshotContent
        to reduce unnecessary CSI object sync/restore., 'BackupRepository maintenance
        improvements: RecentMaintenance history, restart recapture of running jobs,
        readOnly BSL safeguards, and configurable fullMaintenanceInterval GC modes.',
      Volume Policy can now select volumes based on PVC labels (and richer CSI PV
        attribute filtering)., Per-object resource status restore control via `velero.io/restore-status`
        annotation., Restore helper binaries consolidated into the main Velero image.]
    breaking_changes: ['Potential operational change: hooks now execute within Item
        Blocks and may run in parallel when item-block worker count > 1; any hooks
        that implicitly relied on serialized execution may need review.', 'Packaging
        change: if your deployment/automation referenced a separate restore-helper
        image, update it because restore-helper is now included in `velero/velero`
        image.', 'Windows support has explicit gaps (no fs-backup for Windows; no
        Security Descriptors/NTFS extended attributes), which can be a functional
        breaking change if you expected these to work.']
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['**Data mover micro service for CSI snapshot data movement:** Data
        transfer work moves from node-agent pods to dedicated backup/restore pods,
        improving security (no hostPath access), resilience, and giving per-operation
        resource control.', "**Item Block model + ItemBlockAction (IBA) plugin type:**\
        \ Velero can group correlated resources (e.g., Pods/PVCs) into \u201Citem\
        \ blocks\u201D in preparation for future multi-threaded backups; single-thread\
        \ processing remains in 1.15.", '**Configurable node selection for repository
        maintenance jobs:** You can steer repo maintenance jobs to specific nodes
        via a new configMap, helping isolate heavy maintenance workloads.', '**BackupPVC
        configuration options for data movement:** New configMap lets you mount backup
        PVCs read-only (can speed up exposes, e.g., Ceph) and choose the storage class
        used for backupPVCs.', '**Backup repository cache sizing:** New configMap
        lets you cap client-side repository cache usage per repository to avoid ephemeral-storage
        eviction.', '**Performance improvements:** Fixes plugin-call memory leak,
        passes client QPS/burst settings to plugins automatically, and includes upstream
        Kopia improvements that reduce memory use during maintenance.']
    breaking_changes: ['**Restic deprecation begins (fs-backup):** Restic still works
        in 1.15 but emits warnings when `--uploader-type=restic` is used or when the
        Restic path is exercised; plan migration to Kopia.', '**node-agent configMap
        name is no longer fixed:** If you rely on a non-default configMap name, you
        must pass `--node-agent-configmap` so Velero finds it.', '**Repo maintenance
        job settings move from CLI flags to a configMap (preferred):** Values in the
        new repo maintenance configMap take precedence over the equivalent server
        flags (`--keep-latest-maintenance-jobs`, `--maintenance-job-*-request/limit`).',
      '**Changing PVC selected-node feature is deprecated:** Avoid new usage; it will
        be removed in a future release per Velero deprecation policy.']
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Kopia/restic repository maintenance is moved out of the Velero server
        pod into separate Kubernetes Jobs, reducing the chance of OOM during maintenance
        and allowing tuning of job resources at install time.', 'VolumePolicies now
        support action-based handling of volumes (e.g., force fs-backup vs snapshot)
        for more granular opt-in/out control without changing workloads.', Data-mover
        pods can be constrained to run only on eligible nodes via a ConfigMap-driven
        node selection mechanism., 'Restore operations now persist VolumeInfo metadata
        (similar to backup VolumeInfo introduced in 1.13), and `velero restore describe`
        surfaces more CSI/data-movement details.', 'A new Restore phase, `Finalizing`,
        is added to ensure PV labeling and post-restore hooks occur after underlying
        data movement/snapshot restore completes.', 'Azure authentication gains service
        principal certificate-based auth support, in addition to secrets and workload
        identity scenarios.']
    breaking_changes: [CSI plugin is now merged into the Velero repo and installed
        by default as an internal plugin; you must stop installing it via `velero
        install --plugins ...` (and adjust Helm/plugin config accordingly)., Default
        CPU/memory requests/limits for the node-agent are removed so pods run as BestEffort;
        clusters relying on prior defaults may need explicit resources set to avoid
        eviction/CPU starvation., 'Backup namespace filtering changed: if included/excluded
        namespaces are omitted but labelSelector/orLabelSelectors are set, only namespaces
        containing matching resources are included (previously all namespaces were).',
      PV patching in the new Restore `Finalizing` phase can cause restores to end
        `PartiallyFailed` when PVs are stuck Pending; previously such restores might
        have appeared `Complete`.]
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: [Resource Modifiers now support JSON Merge Patch and Strategic Merge
        Patch (in addition to JSON Patch) for more flexible restore-time edits., "Node-agent\
        \ can now limit/scale data movement \u201Cloads\u201D per node (globally and\
        \ per-node) to control CPU/memory/network usage during fs-backups and CSI\
        \ snapshot data mover operations.", Kopia parallel file upload settings are
        now configurable to improve backup/data-mover performance., 'Restores can
        write sparse files for fs-restore and CSI snapshot data movement restores,
        reducing space/time for sparse data.', "`velero backup describe` now includes\
        \ a dedicated \u201CBackup Volumes\u201D section covering native snapshots,\
        \ fs-backups, CSI snapshots, and CSI snapshot data movement; CSI data-movement\
        \ details are now visible.", 'Backups now write a new `VolumeInfo` metadata
        file describing included PVC/PV volume method, snapshot info, and status;
        used to drive PV restore decisions and for downstream summaries.', 'Improved
        resilience for CSI snapshot data movement when Velero server pods or node-agents
        restart, reducing chances of stuck/interrupted operations.', 'Backup/Restore
        hook execution status is now surfaced in CR status and `describe` output (HooksAttempted,
        HooksFailed).', AWS SDK for Go bumped to v2 for better CPU/memory efficiency.,
      Azure AD/Workload Identity is now supported for Kopia operations (fs-backup/data
        mover/etc.) when using the Azure plugin., 'Runtime/deps updated: Go 1.21.6
        and Kopia 0.15.0 (plus library bumps for CVEs).']
    breaking_changes: ["`velero backup describe` output format changed: many volume-related\
        \ fields moved under the new \u201CBackup Volumes\u201D section, which may\
        \ break scripts/parsers.", 'API change: `DataUploadSpec.DataMoverConfig` type
        changed from `*map[string][string]` to `map[string]string` (compile/CRD/client
        compatibility impact).', '`velero install` behavior change: informer cache
        is now enabled by default (previously effectively disabled), which can increase
        memory usage and may require tuning/limits changes or explicitly disabling.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm chart / values changes to plan for\n- **Chart v4.0.0+\
      \ breaks `values.yaml` structure for storage locations**: `configuration.backupStorageLocation`\
      \ (BSL) and `configuration.volumeSnapshotLocation` (VSL) changed **from a map\
      \ to a list/slice** to support **multiple BSLs/VSLs**. This is **not backward\
      \ compatible**.\n  - **Action**: update your Helm values to the new list format\
      \ **before upgrading**.\n  - **Impact**: any templating/overlays (Helmfile/Kustomize/Argo)\
      \ that assume a map will fail.\n\n### Other upgrade-relevant config behavior\
      \ changes (not strictly Helm-only)\n- **Default uploader changed**: `uploader-type`\
      \ default is now **`kopia`** (was `restic`).\n  - **Action**: explicitly set\
      \ uploader type if you require restic behavior/compatibility, or validate Kopia\
      \ in your environment.\n- **CSI snapshot timeout knobs changed**:\n  - Snapshot\
      \ handle creation wait is now controlled by `backup.spec.csiSnapshotTimeout`\
      \ (was fixed 10m in CSI plugin).\n  - Async wait for `ReadyToUse` uses the **operation\
      \ timeout** (default **4h**).\n  - **Action**: review backups that rely on CSI\
      \ snapshots; tune timeouts if needed.\n- **Namespace deletion got riskier due\
      \ to finalizers**: deleting the Velero namespace can hang if controllers are\
      \ gone.\n  - **Action**: use `velero uninstall` (or remove finalizers carefully)\
      \ rather than `kubectl delete ns`.\n"
    chart_updates: [Helm chart v4.0.0+ supports configuring **multiple BackupStorageLocations
        (BSL) and VolumeSnapshotLocations (VSL)**., 'The `configuration.backupStorageLocation`
        and `configuration.volumeSnapshotLocation` values are now **lists**, not maps
        (breaking change).']
    features: ['CSI Snapshot Data Movement: supports moving CSI snapshot data into
        durable backup storage and restoring across environments/clouds.', 'Resource
        Modifiers (JSON substitutions): apply JSON patches to selected resources during
        restore without custom RestoreItemAction plugins.', 'Multiple VolumeSnapshotClasses
        support in the CSI plugin: choose a specific VolumeSnapshotClass per backup
        rather than relying on a single labeled class.', 'Restore finalizer cleanup:
        `velero restore delete` now cleans up restore-associated data in the backup
        storage location.', 'Dependency/runtime updates: Go runtime bumped (1.20.7)
        and Kopia updated (~0.13).']
    breaking_changes: [Default `uploader-type` is now **kopia** instead of restic;
        filesystem backups will use Kopia unless you override., 'CSI snapshot timing
        behavior changed: snapshot handle wait is configurable via `backup.spec.csiSnapshotTimeout`,
        and async `ReadyToUse` waits use operation timeout (default 4h).', Helm chart
        v4.0.0+ changes BSL/VSL configuration from map to list; you must update `values.yaml`
        accordingly before upgrading., 'Finalizers added to Velero CRs (e.g., restore/dataupload/datadownload)
        can cause **namespace deletion to hang** if you delete the namespace directly;
        use `velero uninstall` or clear finalizers first.']
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
