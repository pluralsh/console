icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized fs-backup to a micro-service architecture: adds concurrency
        control, cancel/resume, better resilience to node-agent restarts, and steadier
        resource usage.', fs-backup now supports Windows workloads by running data
        mover pods on Windows nodes (complements CSI snapshot data movement for Windows
        from 1.16)., Beta support for Kubernetes Volume Group Snapshots (VGS) for
        CSI snapshot backups and CSI snapshot data movement to improve write-order
        consistency across multiple volumes., 'PriorityClass support across Velero
        components (server, node-agent, data mover pods, and repository maintenance
        jobs) to control scheduling priority.', 'Scalability/resiliency improvements
        for data movers: configurable node-agent PrepareQueueLength to avoid excessive
        Pending pods; resume/cancel behavior improved across restarts; restore node
        selection and per-storage-class node selection added for CSI snapshot data
        movement.', Resource policy gains reusable include/exclude filters via includeExcludePolicy
        in addition to volumePolicy.]
    breaking_changes: ['Restic uploader path removed: --uploader-type=restic is no
        longer a valid install option; you can still restore from existing Restic-based
        backups until v1.19.', 'Repository maintenance job flags removed from velero
        server (--keep-latest-maintenance-jobs, --maintenance-job-*-request/limit);
        configuration is now via a repository maintenance job ConfigMap.', 'PVC restore
        behavior change: selected-node annotation is always removed when no node mapping
        exists; previously it could be preserved if the node existed.']
  chart_version: 11.3.2
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm values / install-time config changes to check\n\n### 1)\
      \ **Restic uploader removed (breaking)**\n* `--uploader-type=restic` is **no\
      \ longer valid** in v1.17.\n  * If your Helm chart values set anything equivalent\
      \ (commonly `configuration.uploaderType`, `uploaderType`, `deployRestic`, `deployNodeAgent`/`restic`\
      \ toggles, etc.), remove/disable Restic and use the current fs-backup path.\n\
      \  * You can **still restore old Restic-based backups until v1.19**, but you\
      \ cannot create new ones.\n\n### 2) **Repository maintenance job flags removed\
      \ from velero server (breaking)**\nThe following server parameters were removed\
      \ and must be configured via the **repository maintenance job ConfigMap** instead:\n\
      * `--keep-latest-maintenance-jobs`\n* `--maintenance-job-cpu-request`\n* `--maintenance-job-mem-request`\n\
      * `--maintenance-job-cpu-limit`\n* `--maintenance-job-mem-limit`\n\nIf you previously\
      \ set these via chart values that map to `server.extraArgs`, move them to the\
      \ chart\u2019s maintenance-job configmap values (name varies by chart).\n\n\
      ### 3) New/updated **node-agent** configuration knobs worth reviewing\nThese\
      \ are additive but may require Helm values if you want to use them:\n* `PrepareQueueLength`\
      \ (node-agent): throttles creation of data mover pods to avoid large numbers\
      \ stuck Pending.\n* `priorityClassName` support across modules (server, node-agent,\
      \ data mover pods, maintenance jobs): you may want to set these explicitly.\n\
      * Parameterized kubelet mount path for node-agent install (only if you run non-standard\
      \ kubelet paths).\n\n### 4) Modernized fs-backup (architecture change)\nfs-backup\
      \ is now micro-service based. In Helm terms this may translate into:\n* more/changed\
      \ pod templates for data mover / fs-backup components,\n* potential new configmaps/args\
      \ for concurrency/cancel/resume behavior.\n\n(Exact value keys depend on your\
      \ chart; validate rendered manifests before applying.)"
    chart_updates: ['fs-backup moved to a micro-service architecture (better concurrency
        control, cancel/resume, and resiliency across node-agent restarts).', 'Windows
        support expands: fs-backup now supports Windows workloads; data mover pods
        can run on Windows nodes with required tolerations.', Volume Group Snapshots
        support added (Kubernetes beta feature) for CSI snapshot backup and CSI snapshot
        data movement., 'PriorityClass support added across Velero components (server,
        node-agent, data mover pods, maintenance jobs).', 'Data mover scalability
        improvements: throttled pod creation via node-agent `PrepareQueueLength`,
        improved restart/orphan handling, expanded node-selection for restore and
        per-storageclass node-selection.', Resource policy enhanced with reusable
        include/exclude filters via `includeExcludePolicy`., 'Operational changes:
        repository maintenance job configuration moved from server flags to a ConfigMap;
        additional config validation added for install CLI and server start.']
    features: [Modernized fs-backup into a micro-service architecture with concurrency
        control plus cancel/resume and improved resiliency across node-agent restarts.,
      'fs-backup now supports Windows workloads, enabling full Windows backup/restore
        scenarios (with CSI data movement support introduced earlier).', Adds support
        for Kubernetes Volume Group Snapshots to take point-in-time consistent snapshots
        across multiple related volumes., 'Adds PriorityClass support so you can control
        scheduling priority for server, node-agent, data movers, and maintenance jobs.',
      'Improves data mover scalability with a node-agent prepare queue to avoid flooding
        the cluster with Pending pods, plus better restart/orphan handling and node-selection
        (including per-storageclass).', Adds reusable resource include/exclude filtering
        via `includeExcludePolicy` in resource policies.]
    breaking_changes: ["Restic uploader path is removed: `--uploader-type=restic`\
        \ is no longer a valid install configuration; you can\u2019t create new Restic-based\
        \ backups (restores remain supported until v1.19).", Repository maintenance
        job settings are no longer configured via Velero server flags; the maintenance
        job CPU/memory and keep-latest settings must be moved to the repository maintenance
        job ConfigMap., 'PVC restore behavior change: selected-node annotation is
        now removed during PVC restore when no node mapping exists (previously it
        could be preserved in some cases).']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Windows cluster support: Velero can now run in Windows clusters and
        back up/restore Windows workloads; node-agent, data mover pods, and some maintenance
        jobs can be scheduled onto Windows nodes (with stated limitations like no
        fs-backup for Windows).', 'Parallel ItemBlock backup: item blocks can be processed
        concurrently to improve backup throughput; controlled by new server flag `--item-block-worker-count`
        (default 1).', 'Data mover restore scalability: restores for WaitForFirstConsumer
        volumes can be spread across nodes using new node-agent config flag `ignoreDelayBinding`,
        improving parallelism and resource balance.', 'Data mover observability: node-agent
        logs now include more intermediate-object status and cleanup failure details
        for data mover operations, enabled by default.', 'CSI snapshot usability:
        VolumeSnapshotContent objects are no longer unnecessarily retained in backups/restores,
        reducing cross-cluster sync noise.', 'Backup repository maintenance improvements:
        maintenance history (`RecentMaintenance`) added to BackupRepository CR, jobs
        are re-captured after server restarts, maintenance/init skipped for readOnly
        BSLs, and configurable `fullMaintenanceInterval` supports `normalGC/fastGC/eagerGC`.',
      'Volume Policy enhancements: can filter volumes by PVC labels (and additional
        CSI VolumeAttributes properties mentioned in changes).', 'Per-object resource
        status restore: control status restore per object via `velero.io/restore-status`
        annotation.', 'Image packaging: velero, velero-helper, and velero-restore-helper
        are merged into the single Velero image.']
    breaking_changes: []
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Data mover micro service: CSI Snapshot Data Movement now runs in dedicated
        per-backup/per-restore pods instead of node-agent hostPath access, improving
        security isolation, resilience, and enabling per-operation resource sizing.',
      Item Block concepts and new ItemBlockAction (IBA) plugin type to group correlated
        resources for future concurrent processing (v1.15 supports item blocks/IBAs
        but still runs single-threaded)., 'Repository maintenance job node selection
        via a new repository maintenance configuration ConfigMap, allowing maintenance
        jobs to run on designated/idle nodes.', 'Backup PVC configuration for data
        mover: optionally mount backupPVCs read-only (can speed up expose for some
        storage like Ceph) and set storage class for backupPVCs using a backup PVC
        ConfigMap.', Backup repository cache limit configuration via a new backup
        repository configuration ConfigMap to prevent pods from exhausting ephemeral
        storage., 'Performance fixes: plugin-call memory leak fixed; client-qps/client-burst
        inherited by plugins; Kopia improvements to reduce memory during large-repo
        maintenance.']
    breaking_changes: [Restic is deprecated for fs-backup starting in 1.15; workloads
        still work but warnings appear when using --uploader-type=restic or restic-based
        fs-backups/restores., node-agent configuration ConfigMap name is no longer
        fixed; if you customize it you must pass the velero server parameter --node-agent-configmap.,
      'Repository maintenance job settings are moving from velero server flags to
        the new repository maintenance job configuration ConfigMap; ConfigMap values
        take precedence when both are set. Affected flags: --keep-latest-maintenance-jobs,
        --maintenance-job-*-request/limit.', The 'Changing PVC selected-node' feature
        is deprecated in 1.15 and will be removed in a future release; avoid relying
        on it.]
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Repository maintenance for kopia/restic is moved out of the Velero
        server pod into Kubernetes Jobs, reducing risk of Velero pod OOM during maintenance
        and allowing you to tune job resources at install time.', "VolumePolicies\
        \ are extended beyond \u201Cskip\u201D to allow choosing per-volume backup\
        \ method (\u201Cfs-backup\u201D vs \u201Csnapshot\u201D), enabling finer-grained\
        \ opt-in/out behavior without modifying workloads.", 'Data mover backups can
        now constrain where datamover pods run via a ConfigMap-based node selection
        mechanism, helping control resource-heavy jobs and placement.', Restore VolumeInfo
        metadata is now persisted (similar to backup VolumeInfo in 1.13) and `velero
        restore describe` surfaces more detailed volume handling/results., "A new\
        \ Restore phase, \u201CFinalizing\u201D, is introduced to ensure certain actions\
        \ happen after data movement completes (e.g., PV label patching, post-restore\
        \ hooks).", 'Azure plugin gains certificate-based Service Principal authentication
        (in addition to secret-based), enabling phishing-resistant auth aligned with
        Azure recommendations.', 'Dependency/runtime updates: Go 1.22.x and Kopia
        0.17.0; Azure SDK/storage client modernization and AWS SDK updates for identity
        improvements.']
    breaking_changes: [CSI plugin is merged into the main Velero repo and is installed
        by default as an internal plugin; you must stop installing it via `velero
        install --plugins ...` to avoid conflicts/duplication., 'Default CPU/memory
        requests/limits for node-agent are removed, making node-agent pods BestEffort
        by default; you may need to explicitly set resources if you relied on the
        previous defaults.', 'Namespace filtering behavior changes when `includedNamespaces`/`excludedNamespaces`
        are unset but `labelSelector`/`orLabelSelectors` are set: only namespaces
        containing matching resources are backed up (previously all namespaces were
        included).', 'Restores may now end in PartiallyFailed in cases where PV patching
        during the new Finalizing phase is blocked (e.g., PV stuck Pending), whereas
        older versions could report Complete in similar scenarios.']
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Resource Modifiers now support JSON Merge Patch and Strategic Merge
        Patch in addition to JSON Patch, allowing more flexible restore-time resource
        transformations using the same ConfigMap rules.', 'Node-agent can now limit/shape
        how many data-movement loads (fs-backups and CSI snapshot data movement) run
        concurrently per node, configurable globally and per-node.', Kopia uploader
        gained tunables for parallel file upload to improve backup/data-mover throughput
        and overall backup speed., 'Restores via fs-restore and CSI snapshot data
        movement can optionally write sparse files, reducing I/O and speeding restores
        for sparse datasets.', '`velero backup describe` now includes a Backup Volumes
        section summarizing volume handling across native snapshots, fs-backup, CSI
        snapshots and CSI snapshot data movement (including data-movement details
        previously missing).', 'Backups now write a new VolumeInfo metadata file containing
        PVC/PV and snapshot/method/status details, used to drive PV restore behavior
        and enable downstream summary/inspection tooling.', Server and node-agent
        restart behavior was improved so in-progress backup/restore and CSI snapshot
        data-movement operations are less likely to get stuck after restarts., Hook
        execution results are now tracked in Backup/Restore CR status (HooksAttempted/HooksFailed)
        and shown in describe output for better observability., 'AWS integration moved
        to aws-sdk-go-v2, improving CPU/memory efficiency compared to v1.', Azure
        AD/Workload Identity is now supported for Kopia operations (filesystem backups/data
        mover) in addition to prior Azure plugin support for native snapshots.]
    breaking_changes: ['`velero backup describe` output format changed: some existing
        snapshot/fs-backup details were moved into the new Backup Volumes section,
        which may break scripts parsing the old output.', 'API type change: `DataMoverConfig`
        in `DataUploadSpec` changed from `*map[string]string` to `map[string]string`,
        requiring updates for any code/plugins/clients that build these objects.',
      '`velero install` behavior change: informer cache is now enabled by default
        (previously disabled), which can increase memory usage and may require adjusting
        pod resources or explicitly disabling via `--disable-informer-cache` if OOM
        occurs.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "### Helm chart / values changes to address before upgrading\n-\
      \ **Velero Helm chart v4.0.0+ changes BSL/VSL values format (breaking)**: `configuration.backupStorageLocation`\
      \ and `configuration.volumeSnapshotLocation` changed **from a map to a list/slice**\
      \ to support multiple BSLs/VSLs. This is **not backward compatible**, so **convert\
      \ your values to lists before upgrading**.\n- **Uploader default changes**:\
      \ if you rely on defaults, note that Velero\u2019s `uploader-type` default switches\
      \ from **`restic` \u2192 `kopia`** in v1.12. If your Helm chart exposes this\
      \ as a value (or you pass it via `velero install` args), set it explicitly to\
      \ keep current behavior.\n- **Namespace deletion behavior changes due to finalizers**:\
      \ chart uninstall/cleanup should use `velero uninstall` (or ensure finalizers\
      \ are handled) rather than deleting the namespace directly, otherwise deletion\
      \ can hang.\n"
    chart_updates: [Helm chart v4.0.0+ supports configuring **multiple** BackupStorageLocations
        (BSL) and VolumeSnapshotLocations (VSL)., BSL/VSL configuration schema changed
        from **map** to **slice/list** (breaking; requires values migration).]
    features: ['CSI Snapshot Data Movement: moves CSI snapshot data out of the cluster
        into durable backup storage and enables restores across environments/clouds.',
      'Resource Modifiers (JSON Substitutions): apply JSONPatch-style modifications
        to selected resources during restore without writing a RestoreItemAction plugin.',
      'Multiple VolumeSnapshotClasses: choose a specific VolumeSnapshotClass per backup
        instead of relying on driver-name matching + label selection.', 'Restore finalizer:
        `velero restore delete` now also cleans up associated external/backup-location
        restore data, not just the Kubernetes restore CR.']
    breaking_changes: [Default `uploader-type` changed from `restic` to `kopia`; filesystem
        backups will use Kopia unless you explicitly set restic., 'CSI snapshot timing/timeout
        behavior changed: snapshot handle wait is now `backup.spec.csiSnapshotTimeout`
        instead of a fixed 10 minutes; async wait for ReadyToUse uses the operation
        timeout (default 4 hours).', Helm chart v4.0.0+ changes BSL/VSL from map to
        list; you must migrate Helm values before upgrading or the release will fail/misconfigure
        locations., Finalizers added to Velero CRs (restore/dataupload/datadownload)
        can cause **namespace deletion to hang** if you delete the namespace directly;
        use `velero uninstall` or remove finalizers safely.]
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
