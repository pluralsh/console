icon: https://velero.io/img/Velero.svg
git_url: https://github.com/vmware-tanzu/velero
release_url: https://github.com/vmware-tanzu/velero/releases/tag/v{vsn}
helm_repository_url: https://vmware-tanzu.github.io/helm-charts
versions:
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Modernized file system backup (fs-backup) to a micro-service architecture,
        improving robustness, concurrency controls, cancel/resume behavior, and resource
        usage.', fs-backup now supports Windows workloads by allowing data mover pods
        to run on Windows nodes for volume backup/restore., Added (beta) Kubernetes
        VolumeGroupSnapshot support for CSI snapshot backup and CSI snapshot data
        movement to take crash-consistent snapshots across multiple volumes., 'Added
        PriorityClass support across Velero components (server, node-agent, data movers,
        repository maintenance jobs) to control scheduling priority.', 'Improved data
        mover scalability and resiliency: prepare-queue length to throttle pod creation,
        better restart handling and orphan cancellation, and node-selection controls
        (including per storage class) for CSI snapshot data movement restore.', Resource
        policies now support reusable include/exclude filters via includeExcludePolicy
        (in addition to volumePolicy).]
    breaking_changes: ['Restic uploader path is removed: --uploader-type=restic is
        no longer valid for installs/backup creation; restores from existing restic
        backups remain supported only until v1.19.', 'Repository maintenance job tuning
        flags were removed from velero server args and moved to a configmap: --keep-latest-maintenance-jobs
        and maintenance job CPU/memory request/limit flags are no longer accepted.',
      'PVC restore behavior change: selected-node annotation is now always removed
        during PVC restore when no node mapping exists; previously it could be preserved
        if the node existed.']
  chart_version: 11.3.2
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.17.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm values / install-time config changes to check\n\n### 1)\
      \ **Restic uploader removed (breaking)**\n* `--uploader-type=restic` is **no\
      \ longer valid** in v1.17.\n  * If your Helm chart values set anything equivalent\
      \ (commonly `configuration.uploaderType`, `uploaderType`, `deployRestic`, `deployNodeAgent`/`restic`\
      \ toggles, etc.), remove/disable Restic and use the current fs-backup path.\n\
      \  * You can **still restore old Restic-based backups until v1.19**, but you\
      \ cannot create new ones.\n\n### 2) **Repository maintenance job flags removed\
      \ from velero server (breaking)**\nThe following server parameters were removed\
      \ and must be configured via the **repository maintenance job ConfigMap** instead:\n\
      * `--keep-latest-maintenance-jobs`\n* `--maintenance-job-cpu-request`\n* `--maintenance-job-mem-request`\n\
      * `--maintenance-job-cpu-limit`\n* `--maintenance-job-mem-limit`\n\nIf you previously\
      \ set these via chart values that map to `server.extraArgs`, move them to the\
      \ chart\u2019s maintenance-job configmap values (name varies by chart).\n\n\
      ### 3) New/updated **node-agent** configuration knobs worth reviewing\nThese\
      \ are additive but may require Helm values if you want to use them:\n* `PrepareQueueLength`\
      \ (node-agent): throttles creation of data mover pods to avoid large numbers\
      \ stuck Pending.\n* `priorityClassName` support across modules (server, node-agent,\
      \ data mover pods, maintenance jobs): you may want to set these explicitly.\n\
      * Parameterized kubelet mount path for node-agent install (only if you run non-standard\
      \ kubelet paths).\n\n### 4) Modernized fs-backup (architecture change)\nfs-backup\
      \ is now micro-service based. In Helm terms this may translate into:\n* more/changed\
      \ pod templates for data mover / fs-backup components,\n* potential new configmaps/args\
      \ for concurrency/cancel/resume behavior.\n\n(Exact value keys depend on your\
      \ chart; validate rendered manifests before applying.)"
    chart_updates: ['fs-backup moved to a micro-service architecture (better concurrency
        control, cancel/resume, and resiliency across node-agent restarts).', 'Windows
        support expands: fs-backup now supports Windows workloads; data mover pods
        can run on Windows nodes with required tolerations.', Volume Group Snapshots
        support added (Kubernetes beta feature) for CSI snapshot backup and CSI snapshot
        data movement., 'PriorityClass support added across Velero components (server,
        node-agent, data mover pods, maintenance jobs).', 'Data mover scalability
        improvements: throttled pod creation via node-agent `PrepareQueueLength`,
        improved restart/orphan handling, expanded node-selection for restore and
        per-storageclass node-selection.', Resource policy enhanced with reusable
        include/exclude filters via `includeExcludePolicy`., 'Operational changes:
        repository maintenance job configuration moved from server flags to a ConfigMap;
        additional config validation added for install CLI and server start.']
    features: [Modernized fs-backup into a micro-service architecture with concurrency
        control plus cancel/resume and improved resiliency across node-agent restarts.,
      'fs-backup now supports Windows workloads, enabling full Windows backup/restore
        scenarios (with CSI data movement support introduced earlier).', Adds support
        for Kubernetes Volume Group Snapshots to take point-in-time consistent snapshots
        across multiple related volumes., 'Adds PriorityClass support so you can control
        scheduling priority for server, node-agent, data movers, and maintenance jobs.',
      'Improves data mover scalability with a node-agent prepare queue to avoid flooding
        the cluster with Pending pods, plus better restart/orphan handling and node-selection
        (including per-storageclass).', Adds reusable resource include/exclude filtering
        via `includeExcludePolicy` in resource policies.]
    breaking_changes: ["Restic uploader path is removed: `--uploader-type=restic`\
        \ is no longer a valid install configuration; you can\u2019t create new Restic-based\
        \ backups (restores remain supported until v1.19).", Repository maintenance
        job settings are no longer configured via Velero server flags; the maintenance
        job CPU/memory and keep-latest settings must be moved to the repository maintenance
        job ConfigMap., 'PVC restore behavior change: selected-node annotation is
        now removed during PVC restore when no node mapping exists (previously it
        could be preserved in some cases).']
  chart_version: 11.2.0
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.17.1']
- version: 1.16.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Windows cluster support: Velero can now run in Windows/hybrid clusters;
        node-agent, data mover pods, and maintenance jobs can schedule to Windows
        nodes, enabling backup/restore of Windows workloads (with noted limitations).',
      'Parallel Item Block backup: item blocks (and related pre/post hooks) can be
        processed concurrently, improving backup throughput; configurable via the
        new `--item-block-worker-count` server flag (default 1).', 'Data mover restore
        scalability improvement: option to spread restores across nodes for WaitForFirstConsumer
        volumes via new node-agent config `ignoreDelayBinding`, improving parallelism
        and resource balance.', 'Improved observability for data mover operations:
        more detailed logging of intermediate object status and cleanup failures is
        emitted automatically in node-agent logs.', 'CSI snapshot usability improvement:
        unnecessary retained VolumeSnapshotContent objects are no longer included
        in backups/restores, reducing cross-cluster sync noise.', 'Backup repository
        maintenance resiliency/observability: BackupRepository CR gains `RecentMaintenance`
        history; running jobs are recaptured after server restarts; maintenance/initialization
        is skipped for readOnly BSLs; configurable maintenance cadence via `fullMaintenanceInterval`.',
      'Volume Policy enhancement: can now filter volumes by PVC labels.', 'Per-object
        resource status restore: restore status behavior can be controlled per object
        via `velero.io/restore-status` annotation.', 'Image packaging change: velero,
        velero-helper, and velero-restore-helper binaries are merged into a single
        Velero image in v1.16.']
    breaking_changes: ['Restic fs-backup is deprecated starting in v1.15: existing
        restic-based backups/restores still work but generate warnings; plan migration
        to non-restic uploader paths before future removal.', Repository maintenance
        job settings moved from server flags to a dedicated repository-maintenance
        ConfigMap in v1.15; old flags still work but ConfigMap values take precedence
        when both are set., 'node-agent configuration ConfigMap name is no longer
        fixed (v1.15); if you customized config, ensure the `node-agent-configmap`
        server parameter matches your ConfigMap name.']
  chart_version: 10.1.3
  images: ['docker.io/bitnamilegacy/kubectl:1.35', 'velero/velero:v1.16.2']
- version: 1.15.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ["Velero 1.15 introduces a new \u201Cdata mover micro service\u201D\
        \ architecture for CSI snapshot data movement: transfer work moves from node-agent\
        \ pods to dedicated backupPod/restorePod workers, improving isolation, resilience,\
        \ and enabling per-job resource controls.", 'New configuration ConfigMaps
        are introduced for: repository maintenance job scheduling/selection, backup
        PVC behavior (read-only mount + storage class), and backup repository cache
        limits.', Plugins now inherit Velero server client QPS/burst settings automatically;
        memory leak after plugin calls fixed; Kopia repository maintenance memory
        usage improved (upstream fixes included)., Node-agent configMap name is now
        configurable; repository maintenance job settings can be driven via a new
        configMap (server flags still accepted but configMap takes precedence)., 'Additional
        minor changes include labels on maintenance job pods, timezone support in
        cron library, and various restore/backup workflow robustness fixes.']
    features: ['Data mover micro service for CSI snapshot data movement (dedicated
        worker pods) with improved security posture (no hostPath access), better resilience,
        and configurable CPU/memory per transfer job.', 'Item Block concept and new
        ItemBlockAction (IBA) plugin type to group related resources for future multi-threaded
        backup processing (single-threaded execution in 1.15, but the model/plugin
        support is in place).', 'Repository maintenance jobs: ability to select nodes
        for running maintenance jobs via a dedicated configMap so you can steer heavy
        jobs away from critical nodes.', 'BackupPVC configuration: optional read-only
        mounts for data mover backup pods (can speed up expose on some storage like
        Ceph) and ability to set a different StorageClass for backupPVCs.', Backup
        repository cache limit configuration (per repository) to avoid evictions due
        to ephemeral storage usage during repo operations., 'Performance/operability
        improvements: plugin calls memory leak fixed; QPS/burst flags passed through
        to plugins; Kopia maintenance memory improvements; better handling of in-progress
        data movement across node-agent restarts.']
    breaking_changes: ["Restic uploader path for filesystem backups is deprecated\
        \ starting in 1.15: backups/restores still succeed but you\u2019ll see warnings\
        \ when installing with --uploader-type=restic or when using the restic path.\
        \ Plan migration off restic per Velero deprecation policy.", node-agent configuration
        ConfigMap name is no longer assumed/fixed; if you use a non-default configMap
        you must set the node-agent server parameter --node-agent-configmap to point
        to it., 'Repository maintenance job tuning flags are effectively superseded
        by a new repository maintenance job configuration ConfigMap; if both are set,
        the ConfigMap values take precedence. Flags remain for backward compatibility.',
      "\u201CChanging PVC selected-node\u201D feature enters deprecation (not recommended;\
        \ will be removed in a future release)."]
  chart_version: 8.7.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.15.2']
- version: 1.14.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: ['CSI plugin is now built into Velero 1.14 and should no longer
        be installed as an external plugin via `velero install --plugins ...` (applies
        to Helm too: remove separate CSI plugin deployment/sidecar configuration if
        you had it).', 'Node-agent default CPU/memory requests/limits are removed,
        changing QoS to BestEffort unless you set your own resources.', Restore workflow
        adds a new `Finalizing` phase; expect restores to remain InProgress longer
        and PV patching now happens later in the process., Backup namespace-selection
        behavior changes when namespaces are not explicitly included/excluded but
        label selectors are used (namespaces with no matching items are now excluded).,
      Kopia/restic repository maintenance is offloaded from the Velero server pod
        into Kubernetes Jobs; plan RBAC/resources accordingly if you previously relied
        on in-pod maintenance.]
    features: [Repository maintenance for kopia/restic is now executed in separate
        Kubernetes Jobs to reduce Velero server memory spikes and OOMKills., 'VolumePolicies
        now support actions (e.g., `fs-backup` or `snapshot`) to control per-volume
        backup method without changing workloads.', Data mover pods can be scheduled
        only onto eligible nodes via a ConfigMap-based node selection mechanism.,
      'Restore operations now persist per-volume `VolumeInfo` metadata, and `velero
        restore describe` shows more volume details.', Restores include a new `Finalizing`
        phase so post-restore hooks and PV label restoration happen after data movement/snapshot
        restoration completes., Azure plugin adds certificate-based service principal
        authentication support (in addition to secret-based SP auth).]
    breaking_changes: [CSI plugin is merged into the Velero repo and installed by
        default as an internal plugin; do not install it separately or via `--plugins`
        anymore., Node-agent default resource requests/limits were removed; clusters
        depending on those defaults must explicitly set resources to avoid BestEffort
        scheduling/evictions., 'Namespace filtering during backup changes when using
        label selectors without explicit included/excluded namespaces: only namespaces
        containing matching resources will be backed up.', 'PV patching during the
        new Restore `Finalizing` phase can cause restores to become `PartiallyFailed`
        if PVs are stuck `Pending`, whereas they may have reported `Complete` previously.']
  chart_version: 7.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.14.1']
- version: 1.13.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: ''
    chart_updates: []
    features: ['Resource Modifiers enhanced: in 1.13, resource modifier patches support
        JSON Merge Patch and Strategic Merge Patch in addition to JSON Patch, allowing
        more flexible restore-time mutations.', 'Node-agent concurrency controls:
        you can cap/shape how many data movement activities (fs-backups and CSI snapshot
        data movement) run per node, globally or per-node, to manage CPU/memory/network
        impact.', 'Kopia performance/behavior knobs: configurable parallel file upload
        options and an option to write sparse files during restore for fs-restore
        and CSI snapshot data movement.', 'Improved observability/UX: `velero backup
        describe` adds a Backup Volumes section (including CSI snapshot data movement
        info) and backup/restore CR status now includes hook execution counts (attempted/failed)
        surfaced in `describe`.', 'New backup VolumeInfo metadata file: backups now
        include PV/PVC/volume-method metadata used to drive PV restore decisions and
        for downstream tooling summaries.', 'Resilience improvements: backup/restore
        and data movement operations are less likely to get stuck after Velero server
        or node-agent restarts.', 'Cloud/runtime updates: AWS SDK for Go bumped to
        v2 (better CPU/memory), Azure AD/Workload Identity now supported for Kopia
        operations, Go runtime bumped to 1.21.6 and Kopia to 0.15.0.']
    breaking_changes: ['`velero backup describe` output format changed: some existing
        snapshot/fs-backup details moved under the new Backup Volumes section with
        formatting changes; scripts that parse output may break.', 'API type change:
        `DataUploadSpec.dataMoverConfig` changed from `*map[string][string]` to `map[string]string`
        (nil vs empty semantics and client code generation impact).', '`velero install`
        behavior change: informer cache is now enabled by default (previously disabled);
        can increase Velero pod memory usage and may require adjusting limits/requests
        or using `--disable-informer-cache` if you hit OOM.']
  chart_version: 6.7.0
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.13.2']
- version: 1.12.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary:
    helm_changes: "## Helm chart / values changes to plan for\n\n- **Helm chart v4.0.0+\
      \ breaking change (referenced by Velero v1.12 notes):** `backupStorageLocation`\
      \ (BSL) and `volumeSnapshotLocation` (VSL) values **changed from a map/object\
      \ to a list/slice** to support **multiple BSLs and VSLs**. This is **not backward\
      \ compatible**.\n  - **Action:** Update your `values.yaml` to the new list format\
      \ **before** upgrading the chart.\n  - **Impact:** Any existing values like\
      \ `configuration.backupStorageLocation: { ... }` / `configuration.volumeSnapshotLocation:\
      \ { ... }` (or similar) must become arrays, e.g. `configuration.backupStorageLocation:\
      \ [ { ... } ]`.\n\n- **Installer default behavior change (also affects chart\
      \ installs):** `uploader-type` default changes from **restic \u2192 kopia**\
      \ in v1.12.\n  - **Action:** If you rely on restic/node-agent behavior, explicitly\
      \ set uploader type to `restic` during upgrade (or validate kopia compatibility\
      \ and proceed with the new default).\n\n- **Operational note for uninstalls\
      \ (important when managed by Helm):** v1.12 adds finalizers on several Velero\
      \ CRs. **Deleting the namespace can hang**.\n  - **Action:** Prefer `velero\
      \ uninstall` or ensure Velero controllers are running to clear finalizers before\
      \ namespace deletion."
    chart_updates: ["Velero v1.12 references Helm chart v4.0.0+ behavior changes (multi\
        \ BSL/VSL support) and calls out a required values migration (map \u2192 slice)."]
    features: ['CSI Snapshot Data Movement: ability to move CSI snapshot data from
        production/snapshot storage into durable backup storage and restore across
        environments/clouds.', 'Resource Modifiers (JSON Substitutions): apply JSON
        patch-style modifications to selected Kubernetes resources during restore
        without writing a RestoreItemAction plugin.', 'Multiple VolumeSnapshotClasses
        support in the CSI plugin: select a specific VolumeSnapshotClass per backup
        instead of relying on driver/label heuristics.', 'Restore finalizer: `velero
        restore delete` now also cleans up restore-related external data in the backup
        storage location.']
    breaking_changes: [Default uploader type changes from `restic` to `kopia` starting
        in v1.12; environments expecting restic must pin uploader type or validate
        kopia., 'CSI snapshot timing/timeout behavior changes: snapshot handle sync
        wait now uses `backup.spec.csiSnapshotTimeout`; async wait for ReadyToUse
        uses operation timeout (default 4h).', Helm chart v4.0.0+ changes BSL/VSL
        configuration from map to slice for multi-location support; existing values
        must be migrated before upgrade., Finalizers on Velero CRs (restore/dataupload/datadownload)
        can cause Velero namespace deletion to hang if controllers are removed first;
        use `velero uninstall` or clear finalizers carefully.]
  chart_version: 5.2.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.12.3']
- version: 1.11.0
  kube: ['1.35', '1.34', '1.33', '1.32', '1.31', '1.30', '1.29', '1.28', '1.27', '1.26',
    '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19', '1.18']
  requirements: []
  incompatibilities: []
  summary: null
  chart_version: 5.0.2
  images: ['docker.io/bitnami/kubectl:1.35', 'velero/velero:v1.11.1']
- version: 1.10.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.9.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
- version: 1.8.0
  kube: ['1.28', '1.27', '1.26', '1.25', '1.24', '1.23', '1.22', '1.21', '1.20', '1.19',
    '1.18']
  requirements: []
  incompatibilities: []
  summary: null
